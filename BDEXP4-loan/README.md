# é‡‘èå¤§æ•°æ®å®éªŒå››

191870105 åˆ˜å©·

å®Œç»“æ’’èŠ±ğŸ‰



#### å®éªŒç¯å¢ƒåŠè¯­è¨€ï¼š 

1. IDEA JAVA
2. ubuntu + pycharm + pyspark
3. ubuntu + pycharm + pyspark
4. ubuntu + pycharm + pyspark



### ä»»åŠ¡â¼€

> ç¼–å†™ MapReduce ç¨‹åºï¼Œç»Ÿè®¡æ¯ä¸ªâ¼¯ä½œé¢†åŸŸ industry çš„â½¹è´·è®°å½•çš„æ•°é‡ï¼Œå¹¶æŒ‰æ•°é‡ä»â¼¤åˆ°â¼©è¿›â¾æ’åºã€‚
>
> è¾“å‡ºæ ¼å¼ï¼š<â¼¯ä½œé¢†åŸŸ> <è®°å½•æ•°é‡>

#### 1.1 ä¸»è¦æ€è·¯

æœ¬é¢˜ä½¿ç”¨ä¸¤ä¸ªjobå®Œæˆï¼Œç¬¬ä¸€ä¸ªjobè´Ÿè´£æ•°é‡ç»Ÿè®¡ï¼Œç¬¬äºŒè´Ÿè´£æŒ‰ç…§valueæ’åº

```java
// job1
Job solowcjob = Job.getInstance(conf,"solo wordcount");
solowcjob.setJarByClass(WordCount.class);
solowcjob.setMapperClass(SoloTokenizerMapper.class);
solowcjob.setCombinerClass(IntSumReducer.class);
solowcjob.setReducerClass(IntSumReducer.class);
solowcjob.setOutputKeyClass(Text.class);
solowcjob.setOutputValueClass(IntWritable.class);
solowcjob.setOutputFormatClass(SequenceFileOutputFormat.class);
FileInputFormat.addInputPath(solowcjob, new Path(otherArgs.get(0)));// otherArgsçš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¾“å…¥è·¯å¾„
FileOutputFormat.setOutputPath(solowcjob,tempDir);

// job2
Job solosortjob = new Job(conf, "sort");
solosortjob.setJarByClass(WordCount.class);
FileInputFormat.addInputPath(solosortjob,tempDir);
solosortjob.setInputFormatClass(SequenceFileInputFormat.class);
solosortjob.setMapperClass(InverseMapper.class);
solosortjob.setReducerClass(SoloSortReducer.class);
FileOutputFormat.setOutputPath(solosortjob, new Path(otherArgs.get(1)));
solosortjob.setOutputKeyClass(IntWritable.class);
solosortjob.setOutputValueClass(Text.class);
//æ’åºæ”¹å†™æˆé™åº
solosortjob.setSortComparatorClass(IntWritableDecreasingComparator.class);
```

#### 1.2 job1ï¼šSoloTokenizerMapper + IntSumReducer

##### 1.2.1 SoloTokenizerMapper.class

å°†csvæ–‡æœ¬å»é™¤è¡¨å¤´åæŒ‰è¡Œè¯»å–ï¼Œç”¨â€œ,â€è¿›è¡Œåˆ†å‰²ï¼Œå°†linevalue[10]ä½œä¸ºkeyï¼Œ1ä½œä¸ºvalue

è¾“å‡º`<linevalue[10], 1>`ã€‚

```java
public void map(Object key, Text value, Context context)
  throws IOException, InterruptedException {
  String line = value.toString();
  String[] linevalue = line.split(",");
  word.set(linevalue[10]);
  context.write(word, one);
}
```

##### 1.2.1  IntSumReducer.class

æŒ‰ç…§keyå¯¹valueè¿›è¡Œæ±‚å’Œï¼Œè¾“å‡ºä¸º`<key, sum>`

```java
public void reduce(Text key, Iterable<IntWritable> values,
Context context) throws IOException, InterruptedException {
int sum = 0;
for (IntWritable val : values) {
sum += val.get();
}
result.set(sum);
context.write(key, result);
}
```

#### 1.3 job2ï¼šInverseMapper.class + SoloSortReducer.class

##### 1.3.1 InverseMapper.class

å°†<key,value>,è½¬ä¸º<value,key>

##### 1.3.2  SoloSortReducer.class

å°†æŒ‰ç…§valueæ’å¥½åº<value,key>å†™ä¸º<key,value>ï¼ŒæŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåº

```java
public void reduce(IntWritable key,Iterable<Text> values,Context context) throws IOException, InterruptedException {
	for(Text val: values) {
	context.write(val,key);
	}
}
private static class IntWritableDecreasingComparator extends IntWritable.Comparator {
  public int compare(WritableComparable a, WritableComparable b) {
    return -super.compare(a, b);
  }
  public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
    return -super.compare(b1, s1, l1, b2, s2, l2);
  }
}
```

#### 1.4 ç»“æœå±•ç¤º

![2021-12-11 11-31-27å±å¹•æˆªå›¾](/Users/mac/Desktop/exp4/2021-12-11 11-31-27å±å¹•æˆªå›¾.png)



### ä»»åŠ¡â¼†

> ç¼–å†™ Spark ç¨‹åºï¼Œç»Ÿè®¡â½¹ç»œä¿¡â½¤è´·äº§å“è®°å½•æ•°æ®ä¸­æ‰€æœ‰â½¤æˆ·çš„è´·æ¬¾â¾¦é¢ total_loan çš„åˆ†å¸ƒæƒ…å†µã€‚
>
> ä»¥ 1000 å…ƒä¸ºåŒºé—´è¿›â¾è¾“å‡ºã€‚è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š((2000,3000),1234)

#### 2.1 ä¸»è¦æ€è·¯

- åœ¨mapè¿‡ç¨‹ä¸­ï¼ŒæŒ‰è¡Œè¯»å–æ–‡ä»¶ï¼Œé€‰å–æ‰€éœ€è¦çš„total_loanï¼š ```s = x.split(",") s[2]``` ï¼Œå°†total_loanè½¬ä¸ºåŒºé—´ä½œä¸ºkeyï¼Œ1ä½œä¸ºvalue

- reduceè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨``reduceByKey(lambda x,y:x+y)``ï¼Œç»Ÿè®¡ä¸åŒåŒºé—´å‡ºç°æ¬¡æ•°

  ```python
  def map_func(x):
      s = x.split(",")
      total_loan = round(float(s[2]))
      intervalmin = (total_loan // 1000)*1000
      intervalmax = intervalmin + 1000
      interval = "("+str(intervalmin)+","+str(intervalmax)+")"  // åŒºé—´è¡¨ç¤º
      return (interval,1)
  ```

#### 2.2 map + reduceè¿‡ç¨‹ã€‚

```python
lines = sc.textFile("train_data.csv").map(lambda x:map_func(x)).cache()
result = lines.reduceByKey(lambda x,y:x+y).collect()

with open("2.csv","w") as file:
    for i in result:
        file.write("%s%s,%f%s\n" % ("(",i[0],i[1],")"))
file.close()
```

#### 2.3 ç»“æœå±•ç¤º

éƒ¨åˆ†ç»“æœå±•ç¤º,æ‰€æœ‰ç»“æœä½äº2.csv

<img src="/Users/mac/Library/Application Support/typora-user-images/æˆªå±2021-12-18 ä¸‹åˆ8.09.31.png" alt="æˆªå±2021-12-18 ä¸‹åˆ8.09.31" style="zoom:50%;" />

### ä»»åŠ¡ä¸‰

åŸºäº Hive æˆ–è€… Spark SQL å¯¹â½¹ç»œä¿¡â½¤è´·äº§å“è®°å½•æ•°æ®è¿›â¾å¦‚ä¸‹ç»Ÿè®¡ï¼š

> ç»Ÿè®¡æ‰€æœ‰â½¤æˆ·æ‰€åœ¨å…¬å¸ç±»å‹ employer_type çš„æ•°é‡åˆ†å¸ƒå â½æƒ…å†µã€‚
>
> è¾“å‡ºæˆ CSV æ ¼å¼çš„â½‚ä»¶ï¼Œè¾“å‡ºå†…å®¹æ ¼å¼ä¸ºï¼š<å…¬å¸ç±»å‹>,<ç±»å‹å â½>

#### 3.1.1 ä¸»è¦æ€è·¯

- åœ¨RDDä¸Šï¼Œä½¿ç”¨``transformationï¼šmap``æŒ‰è¡Œè¯»å–csvæ–‡ä»¶ï¼Œå¹¶``,``ä¸ºåˆ’åˆ†ä¾æ®å°†å­—ç¬¦ä¸²è¿›è¡Œåˆ’åˆ†ä½œä¸ºkey
- åœ¨RDDæ•°æ®é›†ä¸Šä½¿ç”¨``f(x)``é€‰å–é¢˜ç›®æ‰€éœ€è¦çš„åˆ—ä½¿ç”¨``.toDF()``å°†rddè½¬ä¸ºdataframe 

- åˆ›å»ºè§†å›¾loan

```python
def f(x):
    rel = {}
    rel['loan_id']=x[0]
    rel['employment_type']=x[9]
    return rel
loanDF = sc.textFile('train_data.csv').map(lambda line:line.split(',')).map(lambda x:Row(**f(x))).toDF()
loanDF.createOrReplaceTempView("loan")
```

- ä½¿ç”¨sqlè¯­å¥ï¼Œç»Ÿè®¡employer_typeçš„å æ¯”ï¼Œ
- åœ¨æ‰§è¡Œå®Œsqlè¯­å¥çš„dataframeé€‰å–é¢˜ç›®æ‰€éœ€è¦çš„``<å…¬å¸ç±»å‹>,<ç±»å‹å â½>``
- æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºã€‚

```python
sql = "SELECT employment_type,count(loan_id)/ \
					(select count(loan_id) as all_count \
					 from loan ) 				\
					 as count1 \
			 From loan \
			 GROUP BY employment_type"

loanDF = spark.sql(sql)
loanDF.select("employment_type","count1").write.format("csv").save("3_1.csv")
```

#### 3.1.2 ç»“æœå±•ç¤º

æ–‡ä»¶ä¸º3_1.csv

![æˆªå±2021-12-18 ä¸‹åˆ4.13.36](/Users/mac/Library/Application Support/typora-user-images/æˆªå±2021-12-18 ä¸‹åˆ4.13.36.png)



> ç»Ÿè®¡æ¯ä¸ªâ½¤æˆ·æœ€ç»ˆé¡»ç¼´çº³çš„åˆ©æ¯â¾¦é¢ï¼š
>
> è¾“å‡ºæˆ CSV æ ¼å¼çš„â½‚ä»¶ï¼Œè¾“å‡ºå†…å®¹æ ¼å¼ä¸ºï¼š<user_id>,<total_money>

#### 3.2.1 ä¸»è¦æ€è·¯

- åœ¨RDDä¸Šï¼Œä½¿ç”¨``transformationï¼šmap``æŒ‰è¡Œè¯»å–csvæ–‡ä»¶ï¼Œå¹¶``,``ä¸ºåˆ’åˆ†ä¾æ®å°†å­—ç¬¦ä¸²è¿›è¡Œåˆ’åˆ†ä½œä¸ºkey
- åœ¨RDDæ•°æ®é›†ä¸Šä½¿ç”¨``f(x)``é€‰å–é¢˜ç›®æ‰€éœ€è¦çš„åˆ—ä½¿ç”¨``.toDF()``å°†rddè½¬ä¸ºdataframe 

- åˆ›å»ºè§†å›¾loan

```python
def f(x):
    rel = {}
    rel['loan_id']=x[0]
    rel['user_id'] = x[1]
    rel['year_of_loan']=x[3]
    rel['monthly_payment']=x[5]
    rel['total_loan']=x[2]
    return rel
loanDF = sc.textFile('train_data.csv').map(lambda line:line.split(',')).map(lambda x:Row(**f(x))).toDF()
loanDF.createOrReplaceTempView("loan")
```

- ä½¿ç”¨sqlè¯­å¥ï¼Œç»Ÿè®¡æ¯ä¸ªâ½¤æˆ·æœ€ç»ˆé¡»ç¼´çº³çš„åˆ©æ¯â¾¦é¢
- åœ¨æ‰§è¡Œå®Œsqlè¯­å¥çš„dataframeé€‰å–é¢˜ç›®æ‰€éœ€è¦çš„``<user_id>,<total_money>``
- æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºã€‚

```python
sql2 = "SELECT user_id,year_of_loan*monthly_payment*12 - total_loan as total_money  \
				FROM loan "
loanDF = spark.sql(sql2)
loanDF.select("user_id","total_money").write.format("csv").save("3_2.csv")
```

#### 3.2.2 ç»“æœå±•ç¤º

éƒ¨åˆ†ç»“æœå±•ç¤º,æ‰€æœ‰ç»“æœä½äº3_2.csv

![æˆªå±2021-12-18 ä¸‹åˆ4.20.52](/Users/mac/Library/Application Support/typora-user-images/æˆªå±2021-12-18 ä¸‹åˆ4.20.52.png)



> ç»Ÿè®¡â¼¯ä½œå¹´é™ work_year è¶…è¿‡ 5 å¹´çš„â½¤æˆ·çš„æˆ¿è´·æƒ…å†µ censor_status çš„æ•°é‡åˆ†å¸ƒå â½æƒ…å†µã€‚
>
> è¾“å‡ºæˆ CSV æ ¼å¼çš„â½‚ä»¶ï¼Œè¾“å‡ºå†…å®¹æ ¼å¼ä¸ºï¼š<user_id>,<censor_status>,<work_year>

#### 3.3.1 ä¸»è¦æ€è·¯

- åœ¨RDDä¸Šï¼Œä½¿ç”¨``transformationï¼šmap``æŒ‰è¡Œè¯»å–csvæ–‡ä»¶ï¼Œå¹¶``,``ä¸ºåˆ’åˆ†ä¾æ®å°†å­—ç¬¦ä¸²è¿›è¡Œåˆ’åˆ†ä½œä¸ºkey

- åœ¨RDDæ•°æ®é›†ä¸Šä½¿ç”¨``f(x)``é€‰å–é¢˜ç›®æ‰€éœ€è¦çš„åˆ—ä½¿ç”¨``.toDF()``å°†rddè½¬ä¸ºdataframe 

  - è¿™é‡Œå°†`` xx years ``è½¬ä¸º`` int ``å‹æ•°æ®

    - è½¬åŒ–è§„åˆ™æ˜¯ï¼š

      <1 years : workyear = 0 

      2-9 years : workyear = 2-9

      \>10 years : worker = 10

- åˆ›å»ºè§†å›¾loan

```python
def f(x):
    rel = {}
    rel['loan_id']=x[0]
    rel['user_id'] = x[1]
    workyear = 0
    if x[11]=="":
        workyear = 0
    else:
        year = x[11].split(" ",1)
        if year[0] == "10+":
            workyear = 10
        elif year[0][0] == "<":
            workyear = 0
        else:
            workyear = int(year[0])
    rel['work_year']=workyear
    rel['censor_status']=x[14]
    return rel
loanDF = sc.textFile('train_data.csv').map(lambda line:line.split(',')).map(lambda x:Row(**f(x))).toDF()
loanDF.createOrReplaceTempView("loan")
```

- ä½¿ç”¨sqlè¯­å¥ï¼Œç»Ÿè®¡æ¯ä¸ªâ½¤æˆ·æœ€ç»ˆé¡»ç¼´çº³çš„åˆ©æ¯â¾¦é¢
- åœ¨æ‰§è¡Œå®Œsqlè¯­å¥çš„dataframeé€‰å–é¢˜ç›®æ‰€éœ€è¦çš„``<user_id>,<censor_status>,<work_year>``
- æŒ‰ç…§æ ¼å¼è¦æ±‚è¾“å‡ºã€‚

```python
sql3 = "SELECT user_id,censor_status,work_year F\
				ROM loan \
				Where work_year>5"
loanDF = spark.sql(sql3)
loanDF.select("user_id","censor_status","work_year").write.format("csv").save("3_3.csv")
```

#### 3.3.2 ç»“æœå±•ç¤º

éƒ¨åˆ†ç»“æœå±•ç¤º,æ‰€æœ‰ç»“æœä½äº3_3.csv

![æˆªå±2021-12-18 ä¸‹åˆ4.28.32](/Users/mac/Library/Application Support/typora-user-images/æˆªå±2021-12-18 ä¸‹åˆ4.28.32.png)





### ä»»åŠ¡å››

> æ ¹æ®ç»™å®šçš„æ•°æ®é›†ï¼ŒåŸºäº Spark MLlib æˆ–è€…Spark MLç¼–å†™ç¨‹åºé¢„æµ‹æœ‰å¯èƒ½è¿çº¦çš„å€Ÿè´·â¼ˆï¼Œå¹¶è¯„ä¼°å®éªŒç»“æœçš„å‡†ç¡®ç‡ã€‚

#### 4.1 è¯»å–æ•°æ®

è®¾ç½®å±æ€§`inferSchema=True`ï¼Œpysparkæ ¹æ®è¯»å–åˆ°çš„æ•°æ®å½¢å¼æ¨æ–­æ•°æ®çš„ç±»å‹ã€‚

```python
spark=SparkSession.builder.appName("4").getOrCreate()
df_train = spark.read.csv("test/train_data.csv",header='true',inferSchema='true')
```

#### 4.2 æ•°æ®é¢„å¤„ç†

ï¼ˆ1ï¼‰ç¼ºå¤±å€¼ä»¥-1å¡«å……

```python
df_train = df_train.na.fill(-1)
df_train = df_train.na.fill("-1")
```

ï¼ˆ2ï¼‰æ— å·®åˆ«ç±»åˆ«æ•°æ®ï¼šå°†Stringå‹ç±»åˆ«æ•°æ®ï¼Œå…ˆStringIndexerè½¬ä¸ºindexerï¼Œå†ç”¨OneHotEncoderè½¬ä¸ºonehotç¼–ç ã€‚`work_type`ï¼Œ`employer_type`ï¼Œ`industry`

```Python
# work_type
work_type_stringIndexer = StringIndexer(inputCol="work_type",outputCol="work_type_class",stringOrderType="frequencyDesc")
df_train = work_type_stringIndexer.fit(df_train).transform(df_train)
work_type_encoder = OneHotEncoder(inputCol="work_type_class",outputCol="work_type_onehot").setDropLast(False)
df_train = work_type_encoder.fit(df_train).transform(df_train)

#employer_type
employer_type_stringIndexer = StringIndexer(inputCol="employer_type",outputCol="employer_type_class",stringOrderType="frequencyDesc")
df_train = employer_type_stringIndexer.fit(df_train).transform(df_train)
employer_type_encoder = OneHotEncoder(inputCol="employer_type_class",outputCol="employer_type_onehot").setDropLast(False)
df_train = employer_type_encoder.fit(df_train).transform(df_train)

# industry
industry_stringIndexer = StringIndexer(inputCol="industry",outputCol="industry_class",stringOrderType="frequencyDesc")
df_train = industry_stringIndexer.fit(df_train).transform(df_train)
industry_encoder = OneHotEncoder(inputCol="industry_class",outputCol="industry_onehot").setDropLast(False)
df_train = industry_encoder.fit(df_train).transform(df_train)
```

 ï¼ˆ3ï¼‰æœ‰æ•°å€¼æ„ä¹‰çš„æ•°æ®ï¼šå°†Stringå‹æ•°å€¼å‹æ•°æ®ï¼Œè½¬ä¸ºintå‹ã€‚``work_year`` ï¼Œ``class``ï¼Œ``sub_class``

```Python
@f.udf(returnType = IntegerType())  ## spark.sql éœ€è¦å¥æŸ„
def work_year(x):
    workyear = 0
    if x:
        year = str(x).split(" ",1)
        if year[0] == "10+":
            workyear = 10
        elif year[0][0] == "<":
            workyear = 0
        else:
            workyear = int(year[0])
    return workyear
df_train= df_train.withColumn("work_year",work_year(f.col("work_year")))
```

```python
#class
class_stringIndexer = StringIndexer(inputCol="class",outputCol="class_class")
df_train = class_stringIndexer.fit(df_train).transform(df_train)

#sub_class
subclass_stringIndexer = StringIndexer(inputCol="sub_class",outputCol="subclass_class")
df_train = subclass_stringIndexer.fit(df_train).transform(df_train)
```

ï¼ˆ4ï¼‰æ—¥æœŸæ•°æ®ï¼šä½¿ç”¨datetimeåº“å°†æ—¥æœŸæ•°æ®è½¬ä¸ºç¦»æœ€å°çš„æ—¥æœŸçš„æœˆæ•°ï¼ˆè€ƒè™‘åˆ°æœ¬é¢˜ä¸­æ—¥æœŸæœ€å°é—´éš”ä¸ºæœˆä»½ï¼‰ã€‚``issue_date``,``earlies_credit_mon``

```python
#issue_data
@f.udf(returnType = IntegerType())
def issuedata(x):
    time = 0
    if x == "-1":
        time = 0
    else:
        timeString = x.split("-")
        year = int(timeString[0])
        month = int(timeString[1])
        day = int(timeString[2])
        time1 = datetime(2007, 7, 1)
        time2 = datetime(year, month, day)
        time = (time2-time1).days//30
    return time
df_train= df_train.withColumn("issue_date",issuedata(f.col("issue_date")))
```

ï¼ˆ5ï¼‰å°†é¢„å¤„ç†åçš„æ•°æ®ä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨ã€‚

ï¼ˆ6ï¼‰è¿›è¡Œç‰¹å¾é›†æˆï¼Œå°†æ‰€æœ‰ç‰¹å¾åˆå¹¶åˆ°ä¸€ä¸ªæ•°ç»„featureä¸­ï¼š

```python
featuresArray = ['total_loan', 'year_of_loan', 'interest', 'monthly_payment',\
               'class_class','subclass_class','work_type_onehot','work_year',\
               'employer_type_onehot','industry_onehot','issue_date',\
               'house_exist','house_loan_status',\
               'censor_status','marriage','offsprings','use','post_code',\
               'region','debt_loan_ratio','del_in_18month','scoring_low',\
               'scoring_high','pub_dero_bankrup','early_return','early_return_amount',\
               'early_return_amount_3mon','recircle_b','recircle_u','initial_list_status',\
               'title','policy_code','f0','f1','f2','f3','f4','f5']
assembler = VectorAssembler().setInputCols(featuresArray).setOutputCol("features")
df_train= assembler.transform(df_train)
```

ï¼ˆ7ï¼‰ä½¿ç”¨Standard Sclarizerå°†ç‰¹å¾å‘é‡==æ ‡å‡†åŒ–==ï¼š

```python
scaler = StandardScaler(inputCol="features", outputCol="features_scaled", withMean=True, withStd=True)
df_train = scaler.fit(df_train).transform(df_train)
```

ï¼ˆ8ï¼‰æŒ‰ç…§8ï¼š2çš„æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```python
trainingData, testData = df_train.randomSplit([0.8,0.2])
```

#### 4.3 ç”»å›¾æŸ¥çœ‹éƒ¨åˆ†æ•°æ®ç‰¹å¾

ç»˜åˆ¶ç½‘ç»œè´·æ¬¾ç­‰çº§åˆ†å¸ƒã€ç½‘ç»œè´·æ¬¾é‡‘é¢åˆ†å¸ƒç­‰å›¾

<img src="/Users/mac/Desktop/BDexp4/ç½‘ç»œè´·æ¬¾ç­‰çº§åˆ†å¸ƒ.png" alt="ç½‘ç»œè´·æ¬¾ç­‰çº§åˆ†å¸ƒ" style="zoom:50%;" />

<img src="/Users/mac/Desktop/BDexp4/ç½‘ç»œè´·æ¬¾é‡‘é¢åˆ†å¸ƒ.png" alt="ç½‘ç»œè´·æ¬¾é‡‘é¢åˆ†å¸ƒ" style="zoom:50%;" />

```python
total_y = []
attr = []
for i in range(7):
    attr.append(chr(ord('A')+i))
    total_y.append(df_train.filter(df_train['class'] == attr[i]).count())

pie = (
    Pie()
        .add("ç½‘ç»œè´·æ¬¾ç­‰çº§", [list(z) for z in zip(attr, total_y)])
        .set_global_opts(title_opts=opts.TitleOpts(title="ç½‘ç»œè´·æ¬¾ç­‰çº§åˆ†å¸ƒ"))
        .set_series_opts(
        tooltip_opts=opts.TooltipOpts(trigger="item", formatter="{a} <br/>{b}: {c} ({d}%)"),
        label_opts=opts.LabelOpts(formatter="{b}: {c} ({d}%)")
    )
)

attr = ["0-2000", "2000-4000", "4000-6000","6000-8000","8000-10000",
        "10000-12000","12000-14000","14000-16000","16000-18000","18000-20000",
        "20000-22000","22000-24000","24000-26000","26000-28000","28000-30000",
        "30000-32000","32000-34000","34000-36000","36000-38000","38000-40000"]
money_list = []

for i in range(20):
    money_list.append(df_train.filter((df_train['total_loan'] < (i+1)*2000) & (df_train['total_loan'] >= i*2000)).count())
bar = (
    Bar()
        .add_xaxis(attr)
        .add_yaxis("é¢‘ç‡", money_list)
        .set_global_opts(title_opts=opts.TitleOpts(title="ç½‘ç»œè´·æ¬¾é‡‘é¢åˆ†å¸ƒ"))
)

```

#### 4.4 äºŒåˆ†ç±»è¯„ä»·æŒ‡æ ‡è®¡ç®—å‡½æ•°

```python
def eva_index(Predictions):
    # ä½¿ç”¨æ··æ·†çŸ©é˜µè¯„ä¼°æ¨¡å‹æ€§èƒ½[[TP,FN],[TN,FP]]
    print("-------------------------------------------------------------------")
    print(str(Predictions))
    TP = Predictions.filter(Predictions['prediction'] == 1).filter(Predictions['is_default'] == 1).count()
    FN = Predictions.filter(Predictions['prediction'] == 0).filter(Predictions['is_default'] == 1).count()
    TN = Predictions.filter(Predictions['prediction'] == 0).filter(Predictions['is_default'] == 0).count()
    FP = Predictions.filter(Predictions['prediction'] == 1).filter(Predictions['is_default'] == 0).count()
    # è®¡ç®—æŸ¥å‡†ç‡ TP/ï¼ˆTP+FPï¼‰
    precision = TP/(TP+FP)
    # è®¡ç®—æŸ¥å…¨ç‡ TP/ï¼ˆTP+FNï¼‰
    recall = TP/(TP+FN)
    # è®¡ç®—F1å€¼ 
    F1 =(2 * precision * recall)/(precision + recall)
    # è®¡ç®—å‡†ç¡®ç‡
    acc = (TP + TN) / (TP + FN + TN + FP)
    print("The æŸ¥å‡†ç‡ is :",precision)
    print("The æŸ¥å…¨ç‡ is :",recall)
    print('The F1 is :',F1)
    print('The å‡†ç¡®ç‡ s :', acc)
    # AUCä¸ºrocæ›²çº¿ä¸‹çš„é¢ç§¯ï¼ŒAUCè¶Šæ¥è¿‘ä¸1.0è¯´æ˜æ£€æµ‹æ–¹æ³•çš„çœŸå®æ€§è¶Šé«˜
    auc = BinaryClassificationEvaluator(labelCol="is_default").evaluate(Predictions)
    print("The aucåˆ†æ•° is :",auc)
```

#### 4.5 é€»è¾‘æ–¯è’‚å›å½’åˆ†ç±»

```python
lr = LogisticRegression().setLabelCol("is_default").setFeaturesCol("features_scaled").setMaxIter(10).setRegParam(0.01).\
    setElasticNetParam(0.8).fit(trainingData)
lrPredictions = lr.transform(testData)
eva_index(lrPredictions)
```

æ­£ä¾‹ä¸åä¾‹æƒé‡ç›¸åŒ

The æŸ¥å‡†ç‡ is : 0.7022688356164384

The æŸ¥å…¨ç‡ is : 0.2715385252006952

The F1 is : 0.3916442852879738

The å‡†ç¡®ç‡ s : 0.8298245321134614

The aucåˆ†æ•° is : 0.8424096102126215

#### 4.6 æ”¯æŒå‘é‡æœºåˆ†ç±»

```python
svm = LinearSVC(maxIter=100,labelCol="is_default",featuresCol="features_scaled").fit(trainingData)
svmPredictions = svm.transform(testData)
eva_index(svmPredictions)
```

æ­£ä¾‹ä¸åä¾‹æƒé‡ç›¸åŒ

The æŸ¥å‡†ç‡ is : 0.6916340599962735

The æŸ¥å…¨ç‡ is : 0.3072084747165439

The F1 is : 0.42544412607449855

The å‡†ç¡®ç‡ s : 0.832612651718784

The aucåˆ†æ•° is : 0.847274674473519

#### 4.7 å†³ç­–æ ‘åˆ†ç±»

```python
dt = DecisionTreeClassifier(labelCol="is_default",featuresCol="features_scaled").fit(trainingData)
dtPredictions = dt.transform(testData)
eva_index(dtPredictions)
```

æ­£ä¾‹ä¸åä¾‹æƒé‡ç›¸åŒ

The æŸ¥å‡†ç‡ is : 0.6268525311812179

The æŸ¥å…¨ç‡ is : 0.35355458081602253

The F1 is : 0.4521113345327548

The å‡†ç¡®ç‡ s : 0.8271365844700068

The aucåˆ†æ•° is : 0.6568277818290477

#### 4.8 éšæœºæ£®æ—åˆ†ç±»

```python
rf = RandomForestClassifier(labelCol="is_default",featuresCol="features_scaled",maxBins=700,numTrees=50).fit(trainingData)
rfPredictions = rf.transform(testData)
eva_index(rfPredictions)
```

The æŸ¥å‡†ç‡ is : 0.8282208588957055

The æŸ¥å…¨ç‡ is : 0.011172722006124307

The F1 is : 0.022048015678588925

The å‡†ç¡®ç‡ s : 0.8000567641117251

The aucåˆ†æ•° is : 0.8274599427203987

#### 4.9 ç»“æœå¯¹æ¯”åˆ†æ

å½“æ­£ä¾‹å’Œåä¾‹æƒé‡ç›¸åŒï¼Œæ•°æ®é‡ä¸åŒæ—¶ï¼š

- è™½ç„¶ä»¥ä¸Šç®—æ³•**æŸ¥å‡†ç‡éƒ½æ¯”è¾ƒé«˜**ï¼Œå³é¢„æµ‹ä¸ºè¿çº¦çš„äººä¸­ï¼Œç¡®å®è¿çº¦çš„äººæ¯”ä¾‹è¾ƒé«˜ã€‚
- ä½†æ˜¯å‡ ä¸ªç®—æ³•çš„**æŸ¥å…¨ç‡éƒ½æ¯”è¾ƒä½**ï¼Œå³åœ¨ç¡®å®è¿çº¦çš„äººä¸­ï¼Œè¢«æŸ¥å‡ºæ¥è¿çº¦çš„äººå¾ˆå°‘ã€‚
- å°±è´·æ¬¾è€Œè¨€ï¼Œä¸è‰¯è´·æ¬¾ç‡æ˜¯ç›´æ¥å½±å“é“¶è¡Œç»è¥çŠ¶å†µçš„æŒ‡æ ‡ï¼Œå³å¸Œæœ›ç®—æ³•èƒ½æå‰è¯†åˆ«å‡ºå½“å‰ç”¨æˆ·æ˜¯å¦ä¼šè¿çº¦ï¼Œå¦‚æœè¿çº¦å¯èƒ½æ€§å¾ˆå¤§ï¼Œå®æ„¿ä¸è´·æ¬¾ï¼Œä¹Ÿä¸ä¼šå†’é™©ã€‚å› æ­¤**å¯¹æŸ¥å…¨ç‡è¦æ±‚è¾ƒé«˜**ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è®¾ç½®**äºŒåˆ†ç±»ä»£ä»·çŸ©é˜µ**ã€‚

|              | é¢„æµ‹ç±»åˆ« |      |
| :----------: | :------: | :--: |
| **çœŸå®ç±»åˆ«** |  æœªè¿çº¦  | è¿çº¦ |
|    æœªè¿çº¦    |    0     |  y   |
|     è¿çº¦     |    x     |  0   |

- è§£å†³æ–¹æ³•ï¼šè®¾ç½®æƒé‡ï¼Œè¿™é‡Œä»…éœ€è¦æ§åˆ¶æ¯”å€¼ï¼Œç›¸åŒæ¯”å€¼ä¼šæœ‰ç›¸åŒæ•ˆæœ

##### ==xï¼šy = 4:1==

```python
from pyspark.sql.functions import when
trainingData = trainingData.withColumn("classWeights",when(trainingData.is_default == 1,0.8).otherwise(0.2))
```

åœ¨å››ä¸ªç®—æ³•ä¸­åˆ†åˆ«æ·»åŠ ``weightCol="classWeights"``åå¯ä»¥å¾—åˆ°

\-------------------------------------------------------------------

é€»è¾‘æ–¯è’‚å›å½’

The æŸ¥å‡†ç‡ is : 0.4290086493679308

The æŸ¥å…¨ç‡ is : 0.806672226855713

The F1 is : 0.5601274069784277

The å‡†ç¡®ç‡ s : 0.7464109241452992

The aucåˆ†æ•° is : 0.8477962140118782

\-------------------------------------------------------------------

svm

The æŸ¥å‡†ç‡ is : 0.4308384968573084

The æŸ¥å…¨ç‡ is : 0.8060884070058382

The F1 is : 0.5615431542863782

The å‡†ç¡®ç‡ s : 0.748046875

The aucåˆ†æ•° is : 0.8522481027743516

\-------------------------------------------------------------------

å†³ç­–æ ‘

The æŸ¥å‡†ç‡ is : 0.38212282255683494

The æŸ¥å…¨ç‡ is : 0.8635529608006672

The F1 is : 0.5298060686690885

The å‡†ç¡®ç‡ s : 0.6932091346153846

The aucåˆ†æ•° is : 0.7681579665443818

\-------------------------------------------------------------------

éšæœºæ£®æ—

The æŸ¥å‡†ç‡ is : 0.4363719651855245

The æŸ¥å…¨ç‡ is : 0.7944954128440367

The F1 is : 0.5633353045535187

The å‡†ç¡®ç‡ s : 0.7534722222222222

The aucåˆ†æ•° is : 0.8494405252697155

- å¯ä»¥çœ‹åˆ°æŸ¥å…¨ç‡æœ‰äº†å¤§å¹…åº¦çš„æå‡ï¼Œaucåˆ†æ•°åŸºæœ¬ä¸Šéƒ½æœ‰å°å¹…åº¦çš„æå‡ï¼Œå‡†ç¡®ç‡å˜åŒ–ä¸å¤§ï¼ŒåŸºæœ¬ç»´æŒåœ¨80%å·¦å³ï¼ŒF1å› ä¸ºæŸ¥å…¨ç‡çš„å¤§å¹…æå‡ä¹Ÿæœ‰äº†æ˜¾è‘—æé«˜ï¼Œè™½ç„¶æŸ¥å‡†ç‡æœ‰äº†å¤§å¹…åº¦çš„ä¸‹é™ï¼Œä½†æ˜¯æƒé‡çš„è®¾ç½®éƒ¨åˆ†è§£å†³äº†ç”±ç±»åˆ«ä¸å¹³è¡¡å¸¦æ¥çš„é—®é¢˜ï¼Œä¹Ÿè¯æ˜éœ€è¦é«˜æŸ¥å…¨ç‡çš„åº”ç”¨åœºæ™¯è®¾ç½®æƒé‡æ˜¯æœ‰æ•ˆçš„ã€‚

##### ==xï¼šy = 2:1==

\-------------------------------------------------------------------

lr

The æŸ¥å‡†ç‡ is : 0.5222465353756383

The æŸ¥å…¨ç‡ is : 0.6657650042265427

The F1 is : 0.5853368511017799

The å‡†ç¡®ç‡ s : 0.8129253981559095

The aucåˆ†æ•° is : 0.849834836045144

\-------------------------------------------------------------------

svm

The æŸ¥å‡†ç‡ is : 0.5054364332138735

The æŸ¥å…¨ç‡ is : 0.7033812341504649

The F1 is : 0.5882020287703673

The å‡†ç¡®ç‡ s : 0.8046772841575859

The aucåˆ†æ•° is : 0.8542217416467005

\-------------------------------------------------------------------

dt

The æŸ¥å‡†ç‡ is : 0.5357855262108034

The æŸ¥å…¨ç‡ is : 0.5676246830092984

The F1 is : 0.5512457414932478

The å‡†ç¡®ç‡ s : 0.8167141659681475

The aucåˆ†æ•° is : 0.751555822005103

\-------------------------------------------------------------------

rf

The æŸ¥å‡†ç‡ is : 0.5844083526682134

The æŸ¥å…¨ç‡ is : 0.53229078613694

The F1 is : 0.5571333775713339

The å‡†ç¡®ç‡ s : 0.8321709974853311

The aucåˆ†æ•° is : 0.8405251704670197

- å’Œé¢„æƒ³çš„ç›¸åŒï¼Œå’Œ``xï¼šy = 4 : 1``ç›¸æ¯”æŸ¥å‡†ç‡ä¸Šå‡ï¼ŒæŸ¥å…¨ç‡ä¸‹é™ï¼ŒF1ç¨æœ‰æå‡ï¼Œç²¾åº¦åŸºæœ¬æ— å˜åŒ–ï¼Œaucç¨æœ‰æå‡ã€‚è¯´æ˜xï¼šy = 2:1è¯¥æƒ…å†µèƒ½å‡è¡¡æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡ï¼Œå¹¶ä¸”æœ‰è¾ƒå¥½çš„ç²¾åº¦ã€‚

#### 4.10 æ€è€ƒä¸æ”¹è¿›

1. å¯ä»¥æ ¹æ®ä¸åŒçš„åº”ç”¨åœºæ™¯é€‰å–ä¸åŒçš„æŒ‡æ ‡
2. é’ˆå¯¹ä¸åŒçš„æŒ‡æ ‡å¯ä»¥æœ‰ä¸åŒçš„æ”¹è¿›æ–¹æ³•ï¼Œæ¯”å¦‚è¯¥é¢˜æƒ³è¦æé«˜æŸ¥å…¨ç‡åˆ™éœ€è¦è®¾ç½®**äºŒåˆ†ç±»ä»£ä»·çŸ©é˜µ**
3. å¯ä»¥é’ˆå¯¹ä¸åŒçš„è´·æ¬¾ç±»å‹è¿›è¡Œé¢„æµ‹ï¼Œå°†æ•°æ®é›†æŒ‰ç…§classåˆ’åˆ†ï¼Œè®­ç»ƒå‡ºä¸åŒå‚æ•°çš„æ¨¡å‹ï¼Œå¯¹ç›¸åº”çš„è´·æ¬¾ç±»å‹çš„æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚



### pycharmé…ç½®pysparkç¯å¢ƒï¼š

1. å®˜ç½‘å®‰è£…pycharm

2. å®˜ç½‘ä¸‹è½½sparkï¼Œè§£å‹åˆ° /usr/localä¸­

   sparkæ‰“å¼€åŠè¿è¡ŒæˆåŠŸæˆªå›¾

   <img src="/Users/mac/Desktop/exp4/æ‰“å¼€spark.png" alt="æ‰“å¼€spark" style="zoom:50%;" />

   <img src="/Users/mac/Desktop/exp4/Sparkè¿è¡Œpi.png" alt="Sparkè¿è¡Œpi" style="zoom:50%;" />

3. ä¿®æ”¹é…ç½®æ–‡ä»¶``source ~/.bashrc``æ·»åŠ sparkç¯å¢ƒå˜é‡

4. åœ¨pycharmä¸Šçš„project interpreterä¸Šä¸‹è½½py4j

5. æ‰“å¼€projectï¼Œæ‰“å¼€run configurition

6. è®¾ç½®configurition---Environment--- Environment variables ---ç‚¹å‡»â€œ...â€ï¼Œç‚¹å‡»+ï¼Œè¾“å…¥ä¸¤ä¸ªnameï¼Œä¸€ä¸ªæ˜¯SPARK_HOMEï¼Œå¦å¤–ä¸€ä¸ªæ˜¯PYTHONPATHï¼Œè®¾ç½®å®ƒä»¬çš„valuesï¼ŒSPARK_HOMEçš„valueæ˜¯å®‰è£…æ–‡ä»¶å¤¹sparkçš„ç»å¯¹è·¯å¾„ï¼ŒPYTHONPATHçš„valueæ˜¯è¯¥ç»å¯¹è·¯å¾„ï¼python

7. åœ¨perferencesä¸­çš„project structureä¸­ç‚¹å‡»å³è¾¹çš„â€œadd  content rootâ€ï¼Œæ·»åŠ py4j-some-version.zipå’Œpyspark.zipçš„è·¯å¾„ï¼ˆè¿™ä¸¤ä¸ªæ–‡ä»¶éƒ½åœ¨Sparkä¸­çš„pythonæ–‡ä»¶å¤¹ä¸‹ï¼‰

8.  å®Œæˆï¼Œçº¢çº¿æ¶ˆå¤±ï¼Œè¿è¡Œæ­£å¸¸ã€‚

<img src="/Users/mac/Desktop/exp4/pycharmè¿è¡ŒæˆåŠŸ.png" alt="pycharmè¿è¡ŒæˆåŠŸ" style="zoom:50%;" />

